---
title: "HW 6: Forging Your Own Path Toward Understanding Attitudes Toward Scientists Among US Voters"
author: "Kaori Hirano"
date: "07/19/2023"
format: pdf
---

# Packages

```{r load-packages}
library(readr)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(dplyr))
library(patchwork)
suppressPackageStartupMessages(library(glmnet)) # for ridge, LASSO
library(randomForest)
suppressPackageStartupMessages(library(caret))
library(Matrix)
```

# Data

```{r import-data}
#| warning: false
#| message: false
# imports data, sets na to be any of the values
# where the respondent did not give an answer
# note that 'inapplicable' was retained (-1)
anes2020 <- read_csv("data/anes2020.csv", show_col_types = FALSE,
              na = c('-9', '-8', '-7', '-6', '-5', '-4', '-2', '998', '999'))
```

# Exercises

## Variable Selection
```{r variable-selection}
#| warning: false
#| message: false
# importing data and renaming
d <- anes2020 %>% select(V202025, V202029, V202110x, V202158,
                         V202175, V202185, V202187, V202309, V202310, V202312, 
                         V202329, V202429, V202553, V202332, V202173, V202381)
                         
d <- d %>% rename(comment = V202029, protest = V202025 , pres = V202110x, 
                  draf = V202158, journalists = V202175, pp = V202185, 
                  cdc = V202187, understand = V202309, sidc = V202310, 
                  lies = V202312, vacs = V202329, party = V202429, 
                  autism = V202553, cc = V202332, scientists = V202173, 
                  riskv = V202381)

# drops nas after checking the number of NAs in view
d <- d %>% drop_na()

# making variables into factors as needed
cols <- c('autism', 'party', 'riskv', 'pres', 'comment', 'protest',
          'understand', 'sidc', 'lies', 'vacs', 'cc')
d <- d %>% mutate_each_(funs(factor(.)), cols)

# setting reference to neutral option (ex: neither agree nor disagree) when applicable
d$riskv <- relevel(d$riskv, ref = '3')
d$understand <- relevel(d$understand, ref = '3')
d$vacs <- relevel(d$vacs, ref = '3')
d$sidc <- relevel(d$sidc, ref = '3')
d$pres <- relevel(d$pres, ref = '5')
d$party <- relevel(d$party, ref = '7')
```

### Q1
The variables I chose tried to reflect a variety of things that I thought could be related to how people feel toward scientists. I chose some politically oriented variables, such as who and which party they voted for in the presidential election, because this was a highly polarized election and one candidate had very different views on science than another. I also chose a few feelings thermometers about related topics to science, such as Dr. Anthony Fauci, planned parenthood, and the cdc. I then added a few general science knowledge questions, such as if science is easy to understand as a nonexpert and if vaccines cause autism, to see if understanding of science plays a role in feelings toward scientists. The last set of variables I included dealt with how the respondent felt about political issues aligned with science, such as climate change, covid response, and teaching in schools, which would likely have an effect on how scientists are viewed. 

## Data Visualization
```{r outcome-plot}
#| message: false
b <- ggplot(d, aes(x = scientists)) +
  geom_boxplot() +
  labs(title = 'Warmth Toward Scientists', 
       x = 'Warmth Toward Scientists (0-100 degree warmth)')

h <- ggplot(d, aes(x = scientists)) +
  geom_histogram() +
  labs(title = 'Warmth Toward Scientists', 
       x = 'Warmth Toward Scientists (0-100 degree warmth)')

b + h
```
The boxplot above helps visualize the distribution on feelings toward scientists. We can see that from the 25th percentile and above there are feelings above a 65 warmness rating, while there are a few outliers in the below 25 warmth rating composing a small part of the data. The median warmth appears to be around 85. I also included a histogram, which shows similar information to the boxplot and adds more information regarding the specific breakdown of values within Q1 to Q3, with many responses at/around 50, 60, 70, 80, and 100. 

### Q3 - Predictor and Outcome
```{r predictor-outcome}
s <- ggplot(d, aes(autism, scientists)) +
  geom_point() + 
  geom_jitter() + 
  theme_classic() +
  labs(title = 'Warmth toward scientists by belief that vaccines cause autism',
       y = 'Warmth toward Scientists (0-100 warmth scale)',
       x = 'Belief that Vaccines Cause Autism (0, no; 1, yes)')

b <- ggplot(d, aes(fill = autism, scientists)) +
  geom_bar() +
  theme_classic() + 
  labs(title = 'Warmth toward scientists by belief that vaccines cause autism',
       x = 'Warmth toward Scientists (0-100 warmth scale)',
       color = 'Belief that Vaccines Cause Autism (0, no; 1, yes)')

s + b
```
The plot shows that there is not a clear relationship between believing commonly known scientifically incorrect information and having warm feelings toward scientists. There is little to no evidence suggesting that vaccines cause autism, so this plot puts the belief in this incorrect idea against warmth toward scientists. Surprisingly, there are people with very high trust in scientists that still believe this claim, not just among people with less warmth toward scientists. I chose a bar plot to represent this relationship because it was important to see the breakdown at various levels of warmth, which was less clear in a scatterplot than in the barplot. 

### Q4 - Two Predictors
```{r two-predictors}
ggplot(d, aes(x = draf, y = cdc, color = scientists))+
  geom_point() + 
  geom_jitter() + 
  geom_smooth() +
  labs(title = "Feelings toward Fauci by Feelings toward the CDC with 
       warmth toward Scientists", 
       x = 'Feelings toward Fauci (0-100 warmth scale)',
       y = 'Feelings toward the CDC (0-100 warmth scale)',
       color = 'Feelings toward scientists (0-100 warmth scale)')
```

I chose these variables because I wanted to see if there was a noticable influence of perceptions of Dr. Fauci on perceptions of the CDC because the two could easily be linked together. The scatterplot allows feelings toward both of them to be graphed against each other so any relationship could be more visible. There does not appear to be a relationship between the two, as seen by the large distribution of observations throughout values of warmth and that there is no clear trend between the two predictors in general because the observations are scattered with no clear pattern. 

## Modeling

### Q5 - Choosing Models
I chose random forests and lasso. This is because I want to feel more confident with the random forests method and it is more accurate than a single decision tree and more understandable than a gbm. I also chose lasso because I like that it eliminates variables that do not add enough to the model, which makes interpreting which predictors are important easier. Both of these will point to which predictors are the most important in terms of predicting feelings toward scientists. Random forest's importance plot will point to which variables had the most influence in the response, while lasso will do a similar task by showing the most important but also eliminating variables that had unimportant effects. This will make comparing the two models more feasible because they both are great at identifying the most important predictors in different ways. 

### Q6 - Fitting Models
```{r test-train}
# sets seed 
set.seed(342)

# 70 30 train test split
train <- sample(c(TRUE, FALSE), nrow(d),
     replace = TRUE, prob=c(.7,.3))
test <- (!train)
```

```{r lasso}
# making matrix
x <- model.matrix(scientists ~ ., data = d)[, -1]
y <- d$scientists

# set seed
set.seed(387)

# do cross validation

cv_l <- cv.glmnet(x[train,], y[train], alpha = 1,
lambda = 10^seq(10, -2, length = 100))

# saving optimal lambda
bestlam_l <- cv_l$lambda.min

# calculating MSE
lasso_pred <- predict(cv_l, s = bestlam_l,
newx = x[test, ])
lasso_mse <- mean((lasso_pred - y[test])^2)

# coefficients that matter
lasso_mod <- glmnet(x, y, lambda = bestlam_l)
coef_l <-coef(lasso_mod)
```
```{r rf}
# sets seed
set.seed(286)

# sets training parameters
train_control <- trainControl(method="cv", number = 5)

# gets grid for mtry
tune_grid <- expand.grid(mtry = c(4,5,6,7,16))

# does training
best_forest <- train(scientists ~ ., data = d[train,], 
                     trControl = train_control, 
                     method="rf", 
                     tuneGrid = tune_grid,
                     verbose = FALSE)

# prints to determine best mtry
best_forest

# gets test for y
y_test <- y[test]

# predictions for test set with optimal mtry
rf_sci <- randomForest(scientists ~ ., data = d[train,], mtry = 6,
                       importance = TRUE)
yhat_rf <- predict(rf_sci, newdata = d[test,])

# calculates MSE
rf_mse <- mean((yhat_rf - y_test)^2)

```
For random forests, I used tuning to find the optimal number of variables (mtry) to minimize error. I did this using 5 fold cross validation across 4:7 and 16. These numbers were chosen because the default for regression trees would be around 4/5 given the number of parameters, so I wanted to include a range around that default to find the best number. I also included the full number of predictors to see if bagging was a better option than random forests. 

With LASSO, I used 10 fold cross validation to find the optimal lambda value for the model. This value helps us avoid overfitting the model and is integral in how we reduce the number of variables in LASSO. The optimal number is chosen to minimize error. 

### Q7 - Interpreting Models
```{r comparing-model-important-predictors}
# importances
importances_sci <- importance(rf_sci) %>% 
  as_tibble(rownames = "Variable")

# plot 1
p1_rf <- importances_sci %>% 
  arrange(`%IncMSE`) %>% 
  mutate(Variable = factor(Variable, levels = Variable)) %>% 
  ggplot(aes(x = `%IncMSE`, y = Variable)) +
  geom_col(alpha = 0.5) +
  labs(title = "Variable Importance Plot 1") +
  theme_classic()

# plot 2
p2_rf <- importances_sci %>% 
  arrange(IncNodePurity) %>% 
  mutate(Variable = factor(Variable, levels = Variable)) %>% 
  ggplot(aes(x = IncNodePurity, y = Variable)) +
  geom_col(alpha = 0.5) +
  labs(title = "Variable Importance Plot 2") +
  theme_classic()

# side by side
p1_rf + p2_rf

# gets lasso output ordered nicely
# get coefs into matrix form
coef_matrix <- as.matrix(Matrix::Matrix(coef(lasso_mod, s = "lambda.min")))

# gets abs because we care about overall importance
abs_coef <- abs(coef_matrix)

# descending order
sorted_coef_matrix <- coef_matrix[order(abs_coef, decreasing = TRUE), ]

# make into df
coef_df <- as.data.frame(sorted_coef_matrix)

# add names
coef_df$Feature <- rownames(coef_df)

# prints
coef_df
```
In Q4 I picked feelings about Dr. Anthony Fauci and feelings about the CDC. LASSO found both of these to be important variables, but they did not have as much of an impact as other predictors, such as beliefs about science in covid response and presidential choice. Both feelings toward Fauci and the CDC were associated with an increase in warmth toward scientists. Random Forests found that feelings toward Fauci and the CDC were among the top most important predictors when considering node purity and MSE. These models tell us that there is likely an important effect of feelings toward Fauci and the CDC on feelings toward scientists, although the magnitude of importance is not clear between the two. 

### Q8 - Comparing Models
```{r comparing-models}
# putting together data of predicted, actual, and model type
dataplot <- data.frame(true_value = c(y[test], y[test]))
dataplot$model_type <- c(rep("Lasso", length(lasso_pred)),
                         rep("Random Forests", length(yhat_rf)))
dataplot$predictions <- c(lasso_pred, yhat_rf)

#plotting predicted vs actual by model type
compare <- ggplot(dataplot, aes(x = predictions, y = true_value,
                                color = model_type)) +
  geom_point(shape = 1) + 
  geom_abline(intercept = 0, slope = 1) +
  labs(x = "Predicted Outcome", y = "Actual Outcome",
       title = 'Comparison of Model Type by Predicted vs Actual', 
       color = 'Model Type') + 
  theme_classic() 

# setting up values for graph of mse
name=c("tree","lasso")
mse_all=c(lasso_mse, rf_mse)
value=c(r2(lasso_pred, y[test]), r2(yhat_rf, y[test]))
compare_data=tibble(name,mse_all,value)

#plots comparison of mse
p2=ggplot(compare_data, aes(x=name, y=mse_all))+
  geom_col()+
  labs(x="Model",y="MSE",title = "Comparing MSE")
compare
p2
```

1) The tree model fits better. I know this because the MSE value for the tree model is lower than that of the LASSO model, meaning it is better at making predictions closer to the true values. This is seen in the bar plot comparing the MSEs directly, but also in the scatterplot which shows the distribution of how accurate a prediction was by its distance from the line. 

2) I chose to compare MSE because this measures the average squared difference between the predicted and actual values, telling us how close a model's predictions are to the actual values, which is important to know when doing a prediction task like this one. The bar plot of MSE and the scatterplot visualize the differences in predictions between model types. 