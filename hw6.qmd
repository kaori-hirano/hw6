---
title: "HW 6: Forging Your Own Path Toward Understanding Attitudes Toward Scientists Among US Voters"
author: "Kaori Hirano"
date: "07/19/2023"
format: pdf
---

# Packages

```{r load-packages}
library(readr)
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(dplyr))
library(patchwork)
suppressPackageStartupMessages(library(glmnet)) # for ridge, LASSO
suppressPackageStartupMessages(library(randomForest))
suppressPackageStartupMessages(library(caret))
library(Matrix)
```

# Data

```{r import-data}
#| warning: false
#| message: false
# imports data, sets na to be any of the values
# where the respondent did not give an answer or
# reported unable to answer
# note that 'inapplicable' was retained (-1)
anes2020 <- read_csv("data/anes2020.csv", show_col_types = FALSE,
              na = c('-9', '-8', '-7', '-6', '-5', '-4', '-2', '998', '999'))
```

# Exercises

## Variable Selection
```{r variable-selection}
#| warning: false
#| message: false
# importing data and renaming
d <- anes2020 %>% select(V202025, V202029, V202110x, V202158,
                         V202175, V202185, V202187, V202309, V202310, V202312, 
                         V202329, V202429, V202553, V202332, V202173, V202381)
                         
d <- d %>% rename(comment = V202029, protest = V202025 , pres = V202110x, 
                  draf = V202158, journalists = V202175, pp = V202185, 
                  cdc = V202187, understand = V202309, sidc = V202310, 
                  lies = V202312, vacs = V202329, party = V202429, 
                  autism = V202553, cc = V202332, scientists = V202173, 
                  riskv = V202381)

# drops nas after checking the number of NAs in view
d <- d %>% drop_na()

# making variables into factors as needed
cols <- c('autism', 'party', 'riskv', 'pres', 'comment', 'protest',
          'understand', 'sidc', 'lies', 'vacs', 'cc')
d <- d %>% mutate_each_(funs(factor(.)), cols)

# setting reference to neutral option (ex: neither agree nor disagree) when applicable
d$riskv <- relevel(d$riskv, ref = '3')
d$understand <- relevel(d$understand, ref = '3')
d$vacs <- relevel(d$vacs, ref = '3')
d$sidc <- relevel(d$sidc, ref = '3')
d$pres <- relevel(d$pres, ref = '5')
d$party <- relevel(d$party, ref = '7')
```

### Q1
The variables I chose tried to reflect a variety of things that I thought could be related to how people feel toward scientists. I chose some politically oriented variables, such as who and which party they voted for in the presidential election, because this was a highly polarized election and one candidate appeared to have very different views on science and science-related issues than another. I also chose a few feelings thermometers about related topics to science, such as Dr. Anthony Fauci, planned parenthood, and the cdc. I would expect that people who liked scientists less would have lower scores on these feelings thermometers. I then added a few general science knowledge questions, such as if science is easy to understand as a nonexpert and if vaccines cause autism, because I expected that if people held misconceptions about scientific facts that may like scientists less. The last set of variables I included dealt with how the respondent felt about political issues aligned with science, such as climate change, covid response, and teaching in schools, which would likely have an effect on how scientists are viewed, with a more minimal view about the role of science in these issues being associated with more negative feelings of scientists. 

## Data Visualization
```{r outcome-plot}
#| message: false
#| warning: false
b <- ggplot(d, aes(x = scientists)) +
  geom_boxplot() +
  labs(title = 'Warmth Toward Scientists', 
       x = 'Warmth Toward Scientists (0-100 degree warmth)')

h <- ggplot(d, aes(x = scientists)) +
  geom_histogram() +
  labs(title = 'Warmth Toward Scientists', 
       x = 'Warmth Toward Scientists (0-100 degree warmth)')

b + h
```
The boxplot above helps visualize the distribution on feelings toward scientists, while also making key points in the data (outliers, median, quartiles) clear visually, which is why I chose this plot. We can see that from the 25th percentile and above there are feelings above a 65 warmness rating, while there are a few outliers in the below 25 warmth rating composing a small part of the data. The median warmth appears to be around 85, indicating that a majority of people have overall warm-feelings toward scientists after the 2020 election. 

I also included a histogram, which shows similar information to the boxplot and adds more information regarding the specific breakdown of values within Q1 to Q3, with many responses at/around 50, 60, 70, 80, and 100, suggesting that people may have picked easy common multiples of 10 or 5 in the general range of their feelings as opposed to having reasons for specific numbers. I added this additionally because I wanted to have a clearer picture of the breakdown of specific responses to the question than a boxplot provides. The boxplot is more helpful in general, but this plot adds more information about the spread of the data. 

### Q3 - Predictor and Outcome
```{r predictor-outcome}
s <- ggplot(d, aes(autism, scientists)) +
  geom_point() + 
  geom_jitter() + 
  theme_classic() +
  labs(title = 'Warmth toward scientists by belief that vaccines cause autism',
       y = 'Warmth toward Scientists (0-100 warmth scale)',
       x = 'Belief that Vaccines Cause Autism (0, no; 1, yes)')

b <- ggplot(d, aes(fill = autism, scientists)) +
  geom_bar() +
  theme_classic() + 
  labs(title = 'Warmth toward scientists by belief that vaccines cause autism',
       x = 'Warmth toward Scientists (0-100 warmth scale)',
       color = 'Belief that Vaccines Cause Autism (0, no; 1, yes)')

s + b
```
The plot shows that there is not a clear relationship between believing the myth that vaccines cause autism and having warmer/colder feelings toward scientists. There is little to no evidence suggesting that vaccines cause autism, so this plot puts the belief in this incorrect idea against warmth toward scientists. Surprisingly, there are people with very high trust in scientists that still believe this claim, not just among people with less warmth toward scientists. I did not expect there to be a wide spread of people believing this myth across high to low warmth values. I chose a scatterplot because it makes the overlap between yes and no beliefs about vaccines and autism across degrees of warmth toward scientists very visible for comparison.  

I also added a bar plot to represent this relationship because it was important to see the breakdown at various levels of warmth, which was less clear in a scatterplot than in the barplot. 

### Q4 - Two Predictors
```{r two-predictors}
#| message: false
#| warning: false
ggplot(d, aes(x = draf, y = cdc, color = scientists))+
  geom_point() + 
  geom_jitter() + 
  labs(title = "Feelings toward Fauci by Feelings toward the CDC with 
       warmth toward Scientists", 
       x = 'Feelings toward Fauci (0-100 warmth scale)',
       y = 'Feelings toward the CDC (0-100 warmth scale)',
       color = 'Feelings toward scientists (0-100 warmth scale)') +
  theme(legend.position = "bottom")
```

I chose these variables because I wanted to see if there was a noticeable influence of perceptions of Dr. Fauci on perceptions of the CDC because the two could easily be linked together. I thought that since both are figures that became talked about a lot in 2020 regarding COVID that both were somewhat linked to the government, that there may be a relationship between how people feel about Fauci and the CDC. The scatterplot allows feelings toward both of them to be graphed against each other so any relationship could be more visible. There does not appear to be a relationship between the two, as seen by the large distribution of observations throughout values of warmth and that there is no clear trend between the two predictors in general because the observations are scattered with no clear pattern. I chose a scatterplot. The scatterplot would show a relationship between the two variables by making the distribution visual, which would show a pattern if there was one. It makes it clear that there likely isn't one based on the widely scattered points throughout values of warmth for both predictors. 

## Modeling

### Q5 - Choosing Models
I chose random forests and lasso. Random forests is a more accurate method than single decision trees, and oftentimes more accurate than bagging as well. I wanted to focus on trees more than GBM for this assignment because I can see myself using trees more in the future. I chose lasso because I like that it eliminates variables that do not add enough to the model, which makes interpreting which predictors are important easier than with ridge regression. Both of these methods were chosen because they will point to which predictors are the most important in terms of predicting feelings toward scientists. Random forest's importance plot will point to which variables had the most influence in the response, while lasso will do a similar task by showing the most important but also eliminating variables that had unimportant effects. This will make comparing the two models more feasible because they both are great at identifying the most important predictors in different ways. 

### Q6 - Fitting Models
```{r test-train}
# sets seed 
set.seed(342)

# 70 30 train test split
train <- sample(c(TRUE, FALSE), nrow(d),
     replace = TRUE, prob=c(.7,.3))
test <- (!train)
```

```{r lasso}
# making matrix
x <- model.matrix(scientists ~ ., data = d)[, -1]
y <- d$scientists

# set seed
set.seed(387)

# do cross validation

cv_l <- cv.glmnet(x[train,], y[train], alpha = 1,
lambda = 10^seq(10, -2, length = 100))

# saving optimal lambda
bestlam_l <- cv_l$lambda.min

# calculating MSE
lasso_pred <- predict(cv_l, s = bestlam_l,
newx = x[test, ])
lasso_mse <- mean((lasso_pred - y[test])^2)

# coefficients that matter
lasso_mod <- glmnet(x, y, lambda = bestlam_l)
coef_l <-coef(lasso_mod)
```

```{r rf}
# sets seed
set.seed(286)

# sets training parameters
train_control <- trainControl(method="cv", number = 5)

# gets grid for mtry
tune_grid <- expand.grid(mtry = c(4,5,6,7,8,15))

# does training
best_forest <- train(scientists ~ ., data = d[train,], 
                     trControl = train_control, 
                     method="rf", 
                     tuneGrid = tune_grid,
                     verbose = FALSE)

# prints to determine best mtry
best_forest

# gets test for y
y_test <- y[test]

# predictions for test set with optimal mtry
rf_sci <- randomForest(scientists ~ ., data = d[train,], mtry = 6,
                       importance = TRUE)
yhat_rf <- predict(rf_sci, newdata = d[test,])

# calculates MSE
rf_mse <- mean((yhat_rf - y_test)^2)

```
For random forests, I used tuning to find the optimal number of variables (mtry) to minimize error. I did this using 5 fold cross validation across 4:8 and 16. These numbers were chosen because the default for regression trees would be around 4/5 given the number of parameters, so I wanted to include a range around that default to find the best number. I chose to do CV even though there is a default because, as we saw in the labs, the default value may not always be the best fit. I also included the full number of predictors to see if bagging was a better option than random forests.

With LASSO, I used 10 fold cross validation to find the optimal lambda value for the model. This value helps us avoid overfitting the model and is integral in how we reduce the number of variables in LASSO. The optimal number is chosen to minimize error which is why I used cross validation to compare/minimize errors.

### Q7 - Interpreting Models
```{r comparing-model-important-predictors}
# importances
importances_sci <- importance(rf_sci) %>% 
  as_tibble(rownames = "Variable")

# plot 1
p1_rf <- importances_sci %>% 
  arrange(`%IncMSE`) %>% 
  mutate(Variable = factor(Variable, levels = Variable)) %>% 
  ggplot(aes(x = `%IncMSE`, y = Variable)) +
  geom_col(alpha = 0.5) +
  labs(title = "Variable Importance Plot 1") +
  theme_classic()

# plot 2
p2_rf <- importances_sci %>% 
  arrange(IncNodePurity) %>% 
  mutate(Variable = factor(Variable, levels = Variable)) %>% 
  ggplot(aes(x = IncNodePurity, y = Variable)) +
  geom_col(alpha = 0.5) +
  labs(title = "Variable Importance Plot 2") +
  theme_classic()

# side by side
p1_rf + p2_rf

# gets lasso output ordered nicely
# get coefs into matrix form
coef_matrix <- as.matrix(Matrix::Matrix(coef(lasso_mod, s = "lambda.min")))

# gets abs because we care about overall importance
abs_coef <- abs(coef_matrix)

# descending order
sorted_coef_matrix <- coef_matrix[order(abs_coef, decreasing = TRUE), ]

# make into df
coef_df <- as.data.frame(sorted_coef_matrix)

# add names
coef_df$Feature <- rownames(coef_df)

# prints
coef_df
```
In Q4 I picked feelings about Dr. Anthony Fauci and feelings about the CDC. LASSO found both of these to be important variables, but they did not have as much of an impact as other predictors, such as beliefs about science in covid response and presidential choice. Both feelings toward Fauci and the CDC were associated with an increase in warmth toward scientists. Random Forests found that feelings toward Fauci and the CDC were among the top most important predictors when considering node purity and MSE. These models tell us that there is likely an important effect of feelings toward Fauci and the CDC on feelings toward scientists, with an increase in warm feelings toward Fauci or the CDC increasing feelings towards scientists, although the level of importance warrants further investigation. 

### Q8 - Comparing Models
```{r comparing-models}
# putting together data of predicted, actual, and model type
dataplot <- data.frame(true_value = c(y[test], y[test]))
dataplot$model_type <- c(rep("Lasso", length(lasso_pred)),
                         rep("Random Forest", length(yhat_rf)))
dataplot$predictions <- c(lasso_pred, yhat_rf)

#plotting predicted vs actual by model type
#compare <- ggplot(dataplot, aes(x = predictions, y = true_value,
#                                color = model_type)) +
 # geom_point(shape = 1) + 
#  geom_jitter() +
#  geom_abline(intercept = 0, slope = 1) +
#  labs(x = "Predicted Outcome", y = "Actual Outcome",
#       title = 'Comparison of Model Type by Predicted vs Actual', 
#       color = 'Model Type') + 
#  theme_classic() 

# setting up values for graph of mse
name=c("Random Forest","LASSO")
mse_all=c(lasso_mse, rf_mse)
compare_data=tibble(name,mse_all)

#plots comparison of mse
p2=ggplot(compare_data, aes(x=name, y=mse_all))+
  geom_col()+
  labs(x="Model",y="MSE",title = "Comparing MSE")
compare
p2
```

1) The random forest model fits better. I know this because the MSE value for the tree model is lower than that of the LASSO model, meaning it is better at making predictions closer to the true values. This is seen in the bar plot comparing the MSEs directly where the rf model has a lower MSE than the LASSO model. 

2) I chose to compare MSE because this measures the average squared difference between the predicted and actual values, telling us how close a model's predictions are to the actual values. That is important to know when doing a prediction task like this one. The bar plot of MSE displays the differences in prediction ability between model types which make the difference in results clearer than had the output alone been printed in a table or as code. 